{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a077f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "from typing import Tuple, Optional\n",
    "import gym\n",
    "\n",
    "class TrueRandomSequenceDataset(Dataset):\n",
    "    \"\"\"Dataset that generates random sequences for MDP training\"\"\"\n",
    "    def __init__(self, dataframe, sequence_length, max_overlap_ratio=0.9):\n",
    "        self.dataframe = dataframe\n",
    "        self.sequence_length = sequence_length\n",
    "        self.max_overlap_ratio = max_overlap_ratio\n",
    "        self.min_stride = max(1, int(sequence_length * (1 - max_overlap_ratio)))\n",
    "        self.max_start_idx = len(dataframe) - sequence_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return max(10000, self.max_start_idx)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        start_idx = random.randint(0, self.max_start_idx)\n",
    "        end_idx = start_idx + self.sequence_length\n",
    "        sequence = self.dataframe.iloc[start_idx:end_idx].values\n",
    "        return torch.FloatTensor(sequence)\n",
    "\n",
    "class MDPEnvironment:\n",
    "    \"\"\"Custom MDP Environment using sequence data\"\"\"\n",
    "    def __init__(self, sequence_ torch.Tensor, reward_function=None):\n",
    "        \"\"\"\n",
    "        Initialize MDP environment with sequence data\n",
    "        \n",
    "        Args:\n",
    "            sequence_data: Tensor of shape (T, E) - one sequence from dataset\n",
    "            reward_function: Custom reward function (optional)\n",
    "        \"\"\"\n",
    "        self.sequence_data = sequence_data  # Shape: (T, E)\n",
    "        self.T, self.E = sequence_data.shape\n",
    "        self.current_step = 0\n",
    "        self.reward_function = reward_function or self._default_reward_function\n",
    "        \n",
    "        # State space: current observation + additional features\n",
    "        self.observation_dim = self.E + 2  # features + step + time_remaining\n",
    "        \n",
    "    def reset(self) -> torch.Tensor:\n",
    "        \"\"\"Reset environment to initial state\"\"\"\n",
    "        self.current_step = 0\n",
    "        return self._get_observation()\n",
    "    \n",
    "    def step(self, action: int) -> Tuple[torch.Tensor, float, bool, dict]:\n",
    "        \"\"\"\n",
    "        Take action and return (next_state, reward, done, info)\n",
    "        \n",
    "        Args:\n",
    "            action: Action to take (0: hold, 1: buy, 2: sell for trading example)\n",
    "        \"\"\"\n",
    "        # Get current observation\n",
    "        current_obs = self._get_observation()\n",
    "        \n",
    "        # Calculate reward based on action and current state\n",
    "        reward = self.reward_function(current_obs, action, self.current_step, self.sequence_data)\n",
    "        \n",
    "        # Advance time step\n",
    "        self.current_step += 1\n",
    "        \n",
    "        # Check if episode is done\n",
    "        done = self.current_step >= self.T - 1\n",
    "        \n",
    "        # Get next observation\n",
    "        next_obs = self._get_observation() if not done else torch.zeros(self.observation_dim)\n",
    "        \n",
    "        info = {\n",
    "            'current_step': self.current_step,\n",
    "            'action': action,\n",
    "            'raw_data': self.sequence_data[self.current_step] if not done else None\n",
    "        }\n",
    "        \n",
    "        return next_obs, reward, done, info\n",
    "    \n",
    "    def _get_observation(self) -> torch.Tensor:\n",
    "        \"\"\"Get current observation\"\"\"\n",
    "        if self.current_step >= self.T:\n",
    "            return torch.zeros(self.observation_dim)\n",
    "        \n",
    "        # Current features\n",
    "        current_features = self.sequence_data[self.current_step]\n",
    "        \n",
    "        # Additional MDP features\n",
    "        step_feature = torch.tensor([float(self.current_step) / self.T])  # Normalized step\n",
    "        time_remaining = torch.tensor([float(self.T - self.current_step) / self.T])  # Normalized time remaining\n",
    "        \n",
    "        # Combine all features\n",
    "        observation = torch.cat([current_features, step_feature, time_remaining], dim=0)\n",
    "        return observation\n",
    "    \n",
    "    def _default_reward_function(self, observation, action, step, sequence_data):\n",
    "        \"\"\"Default reward function - customize based on your problem\"\"\"\n",
    "        # Example: Simple trading reward function\n",
    "        if step >= len(sequence_data) - 1:\n",
    "            return 0.0\n",
    "        \n",
    "        current_price = sequence_data[step, 0]  # Assume first feature is price\n",
    "        next_price = sequence_data[step + 1, 0] if step + 1 < len(sequence_data) else current_price\n",
    "        \n",
    "        price_change = (next_price - current_price) / current_price\n",
    "        \n",
    "        # Simple trading logic: \n",
    "        # action 0 = hold, action 1 = buy, action 2 = sell\n",
    "        if action == 1:  # Buy\n",
    "            reward = price_change  # Profit from price increase\n",
    "        elif action == 2:  # Sell\n",
    "            reward = -price_change  # Profit from price decrease\n",
    "        else:  # Hold\n",
    "            reward = 0.0\n",
    "        \n",
    "        return float(reward)\n",
    "\n",
    "class SimplePolicyNetwork(nn.Module):\n",
    "    \"\"\"Simple policy network for the MDP\"\"\"\n",
    "    def __init__(self, observation_dim: int, action_dim: int, hidden_dim: int = 64):\n",
    "        super(SimplePolicyNetwork, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(observation_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, observation: torch.Tensor) -> torch.Tensor:\n",
    "        return self.network(observation)\n",
    "\n",
    "class MDPAgent:\n",
    "    \"\"\"Agent that learns to act in the MDP environment\"\"\"\n",
    "    def __init__(self, observation_dim: int, action_dim: int, learning_rate: float = 0.001):\n",
    "        self.policy_network = SimplePolicyNetwork(observation_dim, action_dim)\n",
    "        self.optimizer = optim.Adam(self.policy_network.parameters(), lr=learning_rate)\n",
    "        self.action_dim = action_dim\n",
    "    \n",
    "    def select_action(self, observation: torch.Tensor, deterministic: bool = False) -> int:\n",
    "        \"\"\"Select action based on current observation\"\"\"\n",
    "        with torch.no_grad():\n",
    "            action_probs = self.policy_network(observation.unsqueeze(0))  # Add batch dimension\n",
    "            if deterministic:\n",
    "                return action_probs.argmax().item()\n",
    "            else:\n",
    "                # Sample action from probability distribution\n",
    "                action = torch.multinomial(action_probs, 1).item()\n",
    "                return action\n",
    "    \n",
    "    def update_policy(self, log_probs: list, rewards: list, gamma: float = 0.99):\n",
    "        \"\"\"Update policy using REINFORCE algorithm\"\"\"\n",
    "        # Calculate discounted returns\n",
    "        returns = []\n",
    "        R = 0\n",
    "        for r in reversed(rewards):\n",
    "            R = r + gamma * R\n",
    "            returns.insert(0, R)\n",
    "        \n",
    "        returns = torch.tensor(returns)\n",
    "        # Normalize returns\n",
    "        if len(returns) > 1:\n",
    "            returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "        \n",
    "        # Calculate policy loss\n",
    "        policy_loss = []\n",
    "        for log_prob, R in zip(log_probs, returns):\n",
    "            policy_loss.append(-log_prob * R)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        policy_loss = torch.stack(policy_loss).sum()\n",
    "        policy_loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "def train_mdp_with_sequences(dataframe, num_epochs: int = 10, batch_size: int = 32):\n",
    "    \"\"\"Train MDP using sequences from TrueRandomSequenceDataset\"\"\"\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    sequence_length = 50\n",
    "    dataset = TrueRandomSequenceDataset(dataframe, sequence_length=sequence_length)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Initialize agent\n",
    "    observation_dim = len(dataframe.columns) + 2  # features + step + time_remaining\n",
    "    action_dim = 3  # hold, buy, sell (example actions)\n",
    "    agent = MDPAgent(observation_dim, action_dim)\n",
    "    \n",
    "    print(f\"Training MDP:\")\n",
    "    print(f\"  Sequence length: {sequence_length}\")\n",
    "    print(f\"  Observation dim: {observation_dim}\")\n",
    "    print(f\"  Action dim: {action_dim}\")\n",
    "    print(f\"  Batch size: {batch_size}\")\n",
    "    print()\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_rewards = []\n",
    "        \n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            batch_rewards = []\n",
    "            \n",
    "            # Process each sequence in the batch\n",
    "            for seq_idx in range(batch.shape[0]):\n",
    "                sequence = batch[seq_idx]  # Shape: (T, E)\n",
    "                \n",
    "                # Create MDP environment for this sequence\n",
    "                env = MDPEnvironment(sequence)\n",
    "                \n",
    "                # Run episode\n",
    "                observation = env.reset()\n",
    "                log_probs = []\n",
    "                rewards = []\n",
    "                \n",
    "                done = False\n",
    "                while not done:\n",
    "                    # Select action\n",
    "                    action = agent.select_action(observation)\n",
    "                    \n",
    "                    # Take action in environment\n",
    "                    next_observation, reward, done, info = env.step(action)\n",
    "                    \n",
    "                    # Store experience\n",
    "                    action_probs = agent.policy_network(observation.unsqueeze(0))\n",
    "                    log_prob = torch.log(action_probs[0, action] + 1e-8)\n",
    "                    log_probs.append(log_prob)\n",
    "                    rewards.append(reward)\n",
    "                    \n",
    "                    # Update observation\n",
    "                    observation = next_observation\n",
    "                \n",
    "                # Update policy with this episode's data\n",
    "                if len(log_probs) > 0:\n",
    "                    agent.update_policy(log_probs, rewards)\n",
    "                \n",
    "                # Track rewards\n",
    "                total_reward = sum(rewards)\n",
    "                batch_rewards.append(total_reward)\n",
    "                epoch_rewards.extend(rewards)\n",
    "            \n",
    "            # Print progress\n",
    "            if batch_idx % 10 == 0:\n",
    "                avg_batch_reward = np.mean(batch_rewards)\n",
    "                print(f\"Epoch {epoch+1}, Batch {batch_idx}: \"\n",
    "                      f\"Avg Reward = {avg_batch_reward:.4f}\")\n",
    "            \n",
    "            # Limit batches per epoch for demo\n",
    "            if batch_idx >= 50:\n",
    "                break\n",
    "        \n",
    "        avg_epoch_reward = np.mean(epoch_rewards)\n",
    "        print(f\"Epoch {epoch+Bruce+1} completed. Average reward: {avg_epoch_reward:.4f}\")\n",
    "    \n",
    "    return agent\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Create sample financial data for demonstration\n",
    "    np.random.seed(42)\n",
    "    timestamps = 1000\n",
    "    price = 100.0\n",
    "    prices = []\n",
    "    \n",
    "    # Generate synthetic price data with trends and noise\n",
    "    for i in range(timestamps):\n",
    "        price_change = np.random.normal(0, 0.02) + 0.0001  # Small upward drift\n",
    "        price = price * (1 + price_change)\n",
    "        prices.append(price)\n",
    "    \n",
    "    # Create DataFrame with multiple features\n",
    "    df = pd.DataFrame({\n",
    "        'price': prices,\n",
    "        'volume': np.random.exponential(1000, timestamps),  # Trading volume\n",
    "        'ma_5': pd.Series(prices).rolling(5).mean().fillna(prices[0]),  # 5-period moving average\n",
    "        'rsi': np.random.uniform(30, 70, timestamps),  # Relative Strength Index\n",
    "        'volatility': np.abs(np.random.normal(0, 0.01, timestamps))  # Volatility measure\n",
    "    })\n",
    "    \n",
    "    print(\"Sample \")\n",
    "    print(df.head())\n",
    "    print(f\"Data shape: {df.shape}\")\n",
    "    print()\n",
    "    \n",
    "    # Train MDP agent\n",
    "    print(\"Starting MDP training...\")\n",
    "    trained_agent = train_mdp_with_sequences(df, num_epochs=3, batch_size=16)\n",
    "    \n",
    "    print(\"\\nTraining completed!\")\n",
    "    print(\"Agent can now be used for inference on new sequences.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
