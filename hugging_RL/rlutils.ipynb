{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478b3fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def categorical_l2_project(\n",
    "    atoms: torch.Tensor,\n",
    "    target_z: torch.Tensor,\n",
    "    target_p: torch.Tensor,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Projects a target distribution onto a fixed set of support points (atoms) using L2 projection.\n",
    "\n",
    "    Args:\n",
    "        atoms: 1D tensor of support points, shape `(num_atoms,)`. Must be sorted in ascending order.\n",
    "        target_z: 2D tensor of target support points, shape `(batch_size, num_target_atoms)`.\n",
    "        target_p: 2D tensor of probabilities for `target_z`, shape `(batch_size, num_target_atoms)`.\n",
    "            Must sum to 1 over the last dimension (valid probability distribution).\n",
    "\n",
    "    Returns:\n",
    "        Projected probabilities over `atoms`, shape `(batch_size, num_atoms)`.\n",
    "    \"\"\"\n",
    "    # Validate input shapes\n",
    "    assert atoms.ndim == 1, f\"atoms must be 1D, got {atoms.ndim}D\"\n",
    "    assert target_z.ndim == 2 and target_p.ndim == 2, \\\n",
    "        f\"target_z and target_p must be 2D, got {target_z.ndim}D and {target_p.ndim}D\"\n",
    "    assert target_z.shape == target_p.shape, \\\n",
    "        f\"target_z and target_p must have the same shape, got {target_z.shape} vs {target_p.shape}\"\n",
    "\n",
    "    num_atoms = atoms.shape[0]\n",
    "    batch_size, num_target_atoms = target_z.shape\n",
    "\n",
    "    # Find indices where target_z would be inserted into atoms (left neighbors)\n",
    "    # Equivalent to JAX's jnp.searchsorted with side=\"left\"\n",
    "    b = torch.searchsorted(atoms, target_z, right=False)  # Shape: (batch_size, num_target_atoms)\n",
    "\n",
    "    # Compute left and right neighbor indices, clamped to valid range [0, num_atoms-1]\n",
    "    l = torch.clamp(b - 1, 0, num_atoms - 1)  # Left neighbors\n",
    "    u = torch.clamp(b, 0, num_atoms - 1)      # Right neighbors\n",
    "\n",
    "    # Get atom values for left and right neighbors (broadcasted to batch)\n",
    "    atoms_l = atoms[l]  # Shape: (batch_size, num_target_atoms)\n",
    "    atoms_u = atoms[u]  # Shape: (batch_size, num_target_atoms)\n",
    "\n",
    "    # Calculate weights for left and right neighbors\n",
    "    eps = 1e-6  # Avoid division by zero\n",
    "    delta_u = target_z - atoms_l  # Distance from target to left neighbor\n",
    "    delta_l = atoms_u - target_z  # Distance from target to right neighbor\n",
    "    \n",
    "    \"\"\"This is the Wasserstein normalization term, which ensures that the weights sum to 1: F^-1(U)\"\"\"\n",
    "    denominator = (atoms_u - atoms_l) + eps  # Normalizer (with epsilon)\n",
    "    # Handle case where left and right neighbors are the same (atoms_l == atoms_u)\n",
    "    # In this case, target_z is exactly on the atom, so assign full weight to that atom\n",
    "    same_neighbor = (atoms_u == atoms_l)\n",
    "    w_l = torch.where(same_neighbor, 1.0, delta_l / denominator)  # Fixed weight logic\n",
    "    w_u = torch.where(same_neighbor, 0.0, delta_u / denominator)\n",
    "    \n",
    "    # Initialize projected probabilities with zeros\n",
    "    p_proj = torch.zeros(\n",
    "        (batch_size, num_atoms),\n",
    "        device=target_z.device,\n",
    "        dtype=target_z.dtype\n",
    "    )\n",
    "\n",
    "    # Accumulate weights into the projected distribution using scatter add\n",
    "    # Add contributions from left neighbors\n",
    "    p_proj.scatter_add_(dim=1, index=l, src=w_l * target_p)\n",
    "    # Add contributions from right neighbors\n",
    "    p_proj.scatter_add_(dim=1, index=u, src=w_u * target_p)\n",
    "\n",
    "    return p_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b78678",
   "metadata": {},
   "outputs": [],
   "source": [
    "#deepmind categorical projection (cramer projection)\n",
    "\n",
    "def categorical_l2_project(\n",
    "    atoms: torch.Tensor,\n",
    "    target_z: torch.Tensor,\n",
    "    target_p: torch.Tensor,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Projects a target distribution onto a fixed set of support points (atoms) using L2 projection on CDFs (CramÃ©r projection).\n",
    "\n",
    "    Args:\n",
    "        atoms: 1D tensor of support points, shape `(num_atoms,)`. Must be sorted in ascending order.\n",
    "        target_z: 2D tensor of target support points, shape `(batch_size, num_target_atoms)`.\n",
    "            Each row must be sorted in ascending order.\n",
    "        target_p: 2D tensor of probabilities for `target_z`, shape `(batch_size, num_target_atoms)`.\n",
    "            Must sum to 1 over the last dimension (valid probability distribution).\n",
    "\n",
    "    Returns:\n",
    "        Projected probabilities over `atoms`, shape `(batch_size, num_atoms)`.\n",
    "    \"\"\"\n",
    "    # Validate input shapes\n",
    "    assert atoms.ndim == 1, f\"atoms must be 1D, got {atoms.ndim}D\"\n",
    "    assert target_z.ndim == 2 and target_p.ndim == 2, \\\n",
    "        f\"target_z and target_p must be 2D, got {target_z.ndim}D and {target_p.ndim}D\"\n",
    "    assert target_z.shape == target_p.shape, \\\n",
    "        f\"target_z and target_p must have the same shape, got {target_z.shape} vs {target_p.shape}\"\n",
    "\n",
    "    batch_size, num_target_atoms = target_z.shape\n",
    "    num_atoms = atoms.shape[0]\n",
    "\n",
    "    # Construct helper arrays from atoms (z_q).\n",
    "    d_pos = torch.roll(atoms, shifts=-1) - atoms  # atoms[i+1] - atoms[i]\n",
    "    d_neg = atoms - torch.roll(atoms, shifts=1)   # atoms[i] - atoms[i-1]\n",
    "\n",
    "    # Clip target_z to be in atoms range [min, max].\n",
    "    target_z = torch.clamp(target_z, atoms[0], atoms[-1]).unsqueeze(1)  # (batch_size, 1, num_target_atoms)\n",
    "\n",
    "    # Get the distance between atom values in support, add dims for broadcast.\n",
    "    d_pos = d_pos.unsqueeze(-1)  # (num_atoms, 1)\n",
    "    d_neg = d_neg.unsqueeze(-1)  # (num_atoms, 1)\n",
    "    atoms_exp = atoms.unsqueeze(-1)  # (num_atoms, 1)\n",
    "\n",
    "    # Ensure no division by zero (for duplicate atoms or boundaries).\n",
    "    d_neg = torch.where(d_neg > 0, 1. / d_neg, torch.zeros_like(d_neg))\n",
    "    d_pos = torch.where(d_pos > 0, 1. / d_pos, torch.zeros_like(d_pos))\n",
    "\n",
    "    # Broadcast target_z and target_p for batch-wise computation over atoms.\n",
    "    delta_qp = target_z - atoms_exp[None, :, :]  # (batch_size, num_atoms, num_target_atoms)\n",
    "    d_sign = (delta_qp >= 0.0).to(target_p.dtype)\n",
    "\n",
    "    # Compute delta_hat: signed distance normalized by directional bin width.\n",
    "    delta_hat = (d_sign * delta_qp * d_pos[None, :, :]) - \\\n",
    "                ((1.0 - d_sign) * delta_qp * d_neg[None, :, :])\n",
    "\n",
    "    target_p = target_p.unsqueeze(1)  # (batch_size, 1, num_target_atoms)\n",
    "\n",
    "    # Projected probs: sum over target atoms.\n",
    "    p_proj = torch.sum(torch.clamp(1.0 - delta_hat, 0.0, 1.0) * target_p, dim=-1)  # (batch_size, num_atoms)\n",
    "\n",
    "    return p_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6672a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def categorical_q_learning(\n",
    "    q_atoms_tm1: torch.Tensor,\n",
    "    q_logits_tm1: torch.Tensor,\n",
    "    a_tm1: int,\n",
    "    r_t: torch.Tensor,\n",
    "    discount_t: torch.Tensor,\n",
    "    q_atoms_t: torch.Tensor,\n",
    "    q_logits_t: torch.Tensor,\n",
    "    stop_target_gradients: bool = True,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Implements Q-learning for categorical Q distributions.\n",
    "\n",
    "    See \"A Distributional Perspective on Reinforcement Learning\", by\n",
    "    Bellemere, Dabney and Munos (https://arxiv.org/pdf/1707.06887.pdf).\n",
    "\n",
    "    Args:\n",
    "        q_atoms_tm1: atoms of Q distribution at time t-1, shape (num_atoms,).\n",
    "        q_logits_tm1: logits of Q distribution at time t-1, shape (num_actions, num_atoms).\n",
    "        a_tm1: action index at time t-1.\n",
    "        r_t: reward at time t, scalar tensor.\n",
    "        discount_t: discount at time t, scalar tensor.\n",
    "        q_atoms_t: atoms of Q distribution at time t, shape (num_atoms,).\n",
    "        q_logits_t: logits of Q distribution at time t, shape (num_actions, num_atoms).\n",
    "        stop_target_gradients: bool indicating whether to apply stop gradient to targets.\n",
    "\n",
    "    Returns:\n",
    "        Categorical Q-learning loss (temporal difference error).\n",
    "    \"\"\"\n",
    "    # Input validation (matching chex assertions)\n",
    "    assert q_atoms_tm1.ndim == 1, f\"q_atoms_tm1 must be 1D, got {q_atoms_tm1.ndim}D\"\n",
    "    assert q_logits_tm1.ndim == 2, f\"q_logits_tm1 must be 2D, got {q_logits_tm1.ndim}D\"\n",
    "    assert isinstance(a_tm1, int), f\"a_tm1 must be int, got {type(a_tm1)}\"\n",
    "    assert r_t.ndim == 0, f\"r_t must be scalar, got {r_t.ndim}D\"\n",
    "    assert discount_t.ndim == 0, f\"discount_t must be scalar, got {discount_t.ndim}D\"\n",
    "    assert q_atoms_t.ndim == 1, f\"q_atoms_t must be 1D, got {q_atoms_t.ndim}D\"\n",
    "    assert q_logits_t.ndim == 2, f\"q_logits_t must be 2D, got {q_logits_t.ndim}D\"\n",
    "\n",
    "    assert q_atoms_tm1.dtype.is_floating_point, \"q_atoms_tm1 must be float\"\n",
    "    assert q_logits_tm1.dtype.is_floating_point, \"q_logits_tm1 must be float\"\n",
    "    assert r_t.dtype.is_floating_point, \"r_t must be float\"\n",
    "    assert discount_t.dtype.is_floating_point, \"discount_t must be float\"\n",
    "    assert q_atoms_t.dtype.is_floating_point, \"q_atoms_t must be float\"\n",
    "    assert q_logits_t.dtype.is_floating_point, \"q_logits_t must be float\"\n",
    "\n",
    "    # Scale and shift time-t distribution atoms by discount and reward\n",
    "    target_z = r_t + discount_t * q_atoms_t\n",
    "\n",
    "    # Convert logits to distribution and find greedy action in state s_t\n",
    "    q_t_probs = F.softmax(q_logits_t, dim=-1)  # Shape: (num_actions, num_atoms)\n",
    "    # Compute mean Q-value for each action: sum(probs * atoms) over atoms\n",
    "    q_t_mean = torch.sum(q_t_probs * q_atoms_t.unsqueeze(0), dim=1)  # Shape: (num_actions,)\n",
    "    pi_t = torch.argmax(q_t_mean)  # Greedy action index\n",
    "\n",
    "    # Get distribution for greedy action\n",
    "    p_target_z = q_t_probs[pi_t]  # Shape: (num_atoms,)\n",
    "\n",
    "    # Project using L2 projection (Cramer distance)\n",
    "    target = categorical_l2_project(target_z, p_target_z, q_atoms_tm1)  # Use PyTorch version of this function\n",
    "\n",
    "    # Stop gradient flow to targets if required\n",
    "    if stop_target_gradients:\n",
    "        target = target.detach()\n",
    "\n",
    "    # Compute cross-entropy loss between target distribution and logits of taken action\n",
    "    logit_qa_tm1 = q_logits_tm1[a_tm1]  # Logits for action a_tm1: (num_atoms,)\n",
    "    # Cross-entropy between target (probabilities) and logits (after softmax)\n",
    "    loss = -torch.sum(target * F.log_softmax(logit_qa_tm1, dim=0))\n",
    "\n",
    "    return loss"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
